{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np # linear algebra\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717495469772
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df=pd.read_csv('azureml://subscriptions/1e06577f-78ae-48c8-8dd1-a55dc756b867/resourcegroups/aditri/workspaces/Code_W_B/datastores/workspaceblobstore/paths/UI/2024-05-30_150256_UTC/emails.csv') #URI path to dataset"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717495485073
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the DataFrame\n",
        "print(test_df.head())\n",
        "\n",
        "# Get an overview of the DataFrame structure\n",
        "print(\"dataset info: \")\n",
        "print(test_df.info())\n",
        "\n",
        "# Get summary statistics of the DataFrame\n",
        "print(test_df.describe())\n",
        "\n",
        "# Check for missing values in the DataFrame\n",
        "print(test_df.isnull().sum())\n",
        "\n",
        "# Display the first 500 entries of the 'file' column\n",
        "print(test_df.file.head(500)) #The 'file' column seems to be an identifier or a path to the email file. It does not provide meaningful information for text analysis or machine learning tasks.\n",
        "\n",
        "# Drop the 'file' column from the DataFrame\n",
        "test_df.drop('file', axis=1, inplace=True)\n",
        "\n",
        "# Split the first message to see its structure and different columns like message_ID, Date, From, To, Subject etc.\n",
        "print(list(test_df.loc[0])[0].split('\\n'))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "                       file                                            message\n0     allen-p/_sent_mail/1.  Message-ID: <18782981.1075855378110.JavaMail.e...\n1    allen-p/_sent_mail/10.  Message-ID: <15464986.1075855378456.JavaMail.e...\n2   allen-p/_sent_mail/100.  Message-ID: <24216240.1075855687451.JavaMail.e...\n3  allen-p/_sent_mail/1000.  Message-ID: <13505866.1075863688222.JavaMail.e...\n4  allen-p/_sent_mail/1001.  Message-ID: <30922949.1075863688243.JavaMail.e...\ndataset info: \n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 517401 entries, 0 to 517400\nData columns (total 2 columns):\n #   Column   Non-Null Count   Dtype \n---  ------   --------------   ----- \n 0   file     517401 non-null  object\n 1   message  517401 non-null  object\ndtypes: object(2)\nmemory usage: 7.9+ MB\nNone\n                         file  \\\ncount                  517401   \nunique                 517401   \ntop     allen-p/_sent_mail/1.   \nfreq                        1   \n\n                                                  message  \ncount                                              517401  \nunique                                             517401  \ntop     Message-ID: <18782981.1075855378110.JavaMail.e...  \nfreq                                                    1  \nfile       0\nmessage    0\ndtype: int64\n0         allen-p/_sent_mail/1.\n1        allen-p/_sent_mail/10.\n2       allen-p/_sent_mail/100.\n3      allen-p/_sent_mail/1000.\n4      allen-p/_sent_mail/1001.\n                 ...           \n495     allen-p/_sent_mail/549.\n496      allen-p/_sent_mail/55.\n497     allen-p/_sent_mail/550.\n498     allen-p/_sent_mail/551.\n499     allen-p/_sent_mail/552.\nName: file, Length: 500, dtype: object\n['Message-ID: <18782981.1075855378110.JavaMail.evans@thyme>', 'Date: Mon, 14 May 2001 16:39:00 -0700 (PDT)', 'From: phillip.allen@enron.com', 'To: tim.belden@enron.com', 'Subject: ', 'Mime-Version: 1.0', 'Content-Type: text/plain; charset=us-ascii', 'Content-Transfer-Encoding: 7bit', 'X-From: Phillip K Allen', 'X-To: Tim Belden <Tim Belden/Enron@EnronXGate>', 'X-cc: ', 'X-bcc: ', \"X-Folder: \\\\Phillip_Allen_Jan2002_1\\\\Allen, Phillip K.\\\\'Sent Mail\", 'X-Origin: Allen-P', 'X-FileName: pallen (Non-Privileged).pst', '', 'Here is our forecast', '', ' ']\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717495487201
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract metadata from the email message\n",
        "def extract_metadata(message):\n",
        "    headers = {}\n",
        "    lines = message.split('\\n')\n",
        "    body_index = 0\n",
        "\n",
        "    # Define a list of common header fields\n",
        "    header_fields = {'Message-ID', 'Date', 'From', 'To', 'Subject', 'Mime-Version', 'Content-Type', 'Content-Transfer-Encoding', 'X-From', 'X-To', 'X-cc', 'X-bcc', 'X-Folder', 'X-Origin', 'X-FileName'}\n",
        "\n",
        "    # Loop through the lines to extract header fields\n",
        "    for i, line in enumerate(lines):\n",
        "        if ':' in line:\n",
        "            key, value = line.split(':', 1)\n",
        "            if key.strip() in header_fields:\n",
        "                headers[key.strip()] = value.strip()\n",
        "                body_index = i\n",
        "            else:\n",
        "                break\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return headers\n",
        "\n",
        "# Function to clean the email body text\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\W', ' ', text)  # Remove non-word characters\n",
        "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)  # Remove single characters\n",
        "    text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', text)  # Remove single characters from start\n",
        "    text = re.sub(r'\\s+', ' ', text, flags=re.I)  # Replace multiple spaces with a single space\n",
        "    text = re.sub(r'^b\\s+', '', text)  # Remove prefixed 'b'\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    return text\n",
        "\n",
        "# Function to extract the body of the email from the message\n",
        "def extract_email_body(message):\n",
        "    try:\n",
        "        body = message.split('\\n\\n', 1)[1]  # Split based on the first double newline\n",
        "    except IndexError:\n",
        "        body = message  # If no double newline, take the entire message\n",
        "    return body\n",
        "\n",
        "# Set of stop words to exclude from the text\n",
        "stop_words = set(ENGLISH_STOP_WORDS)\n",
        "custom_stop_words = {\"original\", \"message\", \"from\", \"subject\", \"com\", \"ect\", \"hou\", \"recipient\"}\n",
        "stop_words.update(custom_stop_words)\n",
        "\n",
        "# Function to preprocess the text by tokenizing and removing stop words\n",
        "def preprocess_text(text):\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text.lower())  # Find all words and convert to lowercase\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Remove stop words\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Function to preprocess the folder names\n",
        "def preprocess_folder(data):\n",
        "    folders = []\n",
        "    for item in data:\n",
        "        if item is None or item == '':\n",
        "            folders.append(np.nan)\n",
        "        else:\n",
        "            item = item.split(\"\\\\\")[-1]\n",
        "            item = item.lower()\n",
        "            folders.append(item)\n",
        "    return folders      \n",
        "\n",
        "# List of keywords commonly found in spam emails\n",
        "spam_keywords = [\n",
        "    'click below', 'click here', 'one hundred percent guaranteed', '50% off', 'unlimited', 'lottery', 'prize', \n",
        "    'claim', 'free', 'win', 'winner', 'congratulations', 'limited time', 'offer', 'discount', 'promotion', \n",
        "    'click here', 'buy now', 'urgent', 'money', 'guarantee', 'risk free', 'deal', 'save big', 'exclusive', \n",
        "    'clearance', 'cheap', 'bargain', 'bonus', 'no cost', 'double your', 'million', 'buy direct', 'clearance', \n",
        "    'make $', 'online degree', 'best price'\n",
        "]\n",
        "\n",
        "# Function to detect if an email is spam based on the presence of spam keywords\n",
        "def is_spam(text):\n",
        "    count = sum(keyword in text for keyword in spam_keywords)\n",
        "    return count > 4\n",
        "\n",
        "# Function to preprocess the entire dataset\n",
        "def preprocess_new_data(df):\n",
        "    # Extract and clean the email body text\n",
        "    unwanted_folders = [\"all documents\", \"deleted items\", \"discussion threads\", \"sent\", \"deleted Items\", \"inbox\",\n",
        "                   \"sent items\", \"'sent mail\", \"untitled\", \"notes inbox\", \"calendar\"]\n",
        "    df['cleaned_body'] = df['message'].apply(lambda x: clean_text(extract_email_body(x)))\n",
        "    df['processed_message'] = df['cleaned_body'].apply(preprocess_text)\n",
        "    metadata = df['message'].apply(extract_metadata)\n",
        "    metadata_df = pd.DataFrame(metadata.tolist())\n",
        "    df = pd.concat([df, metadata_df], axis=1)\n",
        "    df.fillna('', inplace=True)\n",
        "    df['X-Folder'] = preprocess_folder(df['X-Folder'])\n",
        "    df.fillna('', inplace=True)\n",
        "    temp = ~df['X-Folder'].isin(unwanted_folders)\n",
        "    filtered_df = df.loc[temp].copy()\n",
        "    df['category'] = filtered_df['X-Folder']\n",
        "    df.dropna('category',inplace=True)\n",
        "    df['is_spam'] = df['processed_message'].apply(is_spam)\n",
        "    df.fillna('', inplace=True)\n",
        "    return df\n",
        "\n",
        "# Function to categorize emails based on the folder names and spam detection\n",
        "def categorize_label(row):\n",
        "    file_name = row['X-Folder'].lower()\n",
        "    if row['is_spam']:\n",
        "        return 'Spam'\n",
        "    elif any(word in file_name for word in ['junk file','junk','motley fool e-mails','junk e-mail']):\n",
        "        return 'Spam'\n",
        "    elif any(word in file_name for word in['resume','application','interviews','recruiting']):\n",
        "        return 'Recruiting'\n",
        "    elif any(word in file_name for word in ['universities','mba program','stanford','mba--e-commerce','analyst prog']):\n",
        "        return 'Programs'\n",
        "    elif any(word in file_name for word in['meeting','conference','presentations','meetings - tw customer']):\n",
        "        return 'Meetings'\n",
        "    elif any(word in file_name for word in['hr','human resources','netco hr']):\n",
        "        return 'HR'\n",
        "    elif any(word in file_name for word in['enron news', 'corporate comm','enron t&s','corp info_announcements','corp info/announcements','organizational changes','organizational issues','org announc. & chrts','policy','corp memos','restructuring','organizational announcements','org. announc.','newsletter']):\n",
        "        return 'Announcements'\n",
        "    elif any(word in file_name for word in ['genco-jv_ipo','receipts','deal discrepancies','online trading','ctg-deals','iso_ pricecaps','ca refunds','asset marketing','finance','bankrupt','fin desk','credit issues','budget','trading info','2001 budget','finanial operations','trading issues','s.a. trading','accounting','expense reports']):\n",
        "        return 'Finance'\n",
        "    elif any(word in file_name for word in ['savedmail', 'saved mail', 'personal', 'contacts','to do','myfriends','personal mail','quotes','tasks','personalfolder','congratulations','personal stuff','church','humor','social','family','save the date','friends','funny','compliments','pictures','daily blessings','personal contacts','to do list','stuff','fun','fun emails','personal files']):\n",
        "        return 'Personal'\n",
        "    elif any(word in file_name for word in ['tasks', 'memo', 'press releases', 'straw','info' ,'logistics','corporate','management','market','projects','personnel','_exchange_info','work mail']):\n",
        "        return 'Work'\n",
        "    elif any(word in file_name for word in ['california', 'vanderbilt', 'universal studios','portland','europe','canada','yazoo city','denver','australia','mexico','japan','london','india','chicago','florida','brazil','dublin','california - working group','chicago office','paris','pasadena','italy','argentina','guatemala']):\n",
        "        return 'Locations'\n",
        "    elif any(word in file_name for word in ['bmc', 'fedex', 'sony', 'dell', 'hp', 'ge', 'avaya', 'compaq','ces','it','wordsmith']):\n",
        "        return 'Technology'\n",
        "    else:\n",
        "        return 'Other'\n"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717495487418
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the new data\n",
        "test_df = preprocess_new_data(test_df)"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717474681772
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first few rows of the DataFrame to check the kind of values in the dataset\n",
        "print(test_df.head())\n",
        "\n",
        "# Print the list of columns in the DataFrame\n",
        "print(list(test_df))\n",
        "\n",
        "# Check for NA values in the DataFrame\n",
        "print(test_df.isna().sum())\n",
        "\n",
        "# Print unique values in the 'Mime-Version' column\n",
        "print(test_df['Mime-Version'].unique())\n",
        "\n",
        "# Print unique values in the 'Content-Transfer-Encoding' column\n",
        "print(test_df['Content-Transfer-Encoding'].unique())\n",
        "\n",
        "# Print unique values in the 'Content-Type' column\n",
        "print(test_df['Content-Type'].unique())\n",
        "\n",
        "# Drop irrelevant columns from the DataFrame\n",
        "test_df.drop(['message','Mime-Version','Content-Type','X-From','Content-Transfer-Encoding','X-To','X-cc','X-bcc','X-FileName','X-Origin'], axis=1, inplace=True)\n",
        "\n",
        "# Fill empty spaces with an empty string\n",
        "test_df.fillna('', inplace=True)\n",
        "\n",
        "# Print the counts of different categories in the 'category' column\n",
        "print(test_df.category.value_counts())\n",
        "\n",
        "# Save the category counts to a CSV file for easier viewing (not shown in code but implied by the comment)\n",
        "test_df.category.value_counts()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717474683970
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df['label'] = test_df.apply(categorize_label, axis=1)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717474692883
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a mapping from unique labels to numeric indices\n",
        "label_mapping = {label: idx for idx, label in enumerate(test_df['label'].unique())}\n",
        "\n",
        "# Map the labels in the DataFrame to the corresponding numeric indices\n",
        "test_df['label_encoded'] = test_df['label'].map(label_mapping)\n",
        "\n",
        "# Extract the encoded labels as the target variable\n",
        "y = test_df['label_encoded']\n",
        "\n",
        "# Vectorization using Bag of Words with a limited vocabulary size\n",
        "# Combine 'processed_message', 'Subject', and 'X-Folder' columns to form the text data\n",
        "vectorizer = CountVectorizer(max_features=150000)  \n",
        "\n",
        "# Fit the vectorizer on the combined text data and transform the text into numerical features\n",
        "X = vectorizer.fit_transform(test_df['processed_message'] + \" \" + test_df['Subject'] + \" \" + test_df['X-Folder'])\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717474761262
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scaling the data\n",
        "scaler = StandardScaler(with_mean=False)  # with_mean=False to work with sparse matrix\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define Logistic Regression model with 'saga' solver\n",
        "model = LogisticRegression(max_iter=5000, solver='saga', tol=1e-3)\n",
        "\n",
        "# Use a smaller hyperparameter grid\n",
        "params = {'C': [1, 10]}\n",
        "grid = GridSearchCV(model, params, cv=5, n_jobs=-1)\n",
        "grid.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the best estimator\n",
        "best_estimator = grid.best_estimator_\n",
        "print(f\"Best params for LogisticRegression: {grid.best_params_}\")\n",
        "\n",
        "# Evaluate the best estimator on the test set\n",
        "test_score = best_estimator.score(X_test_scaled, y_test)\n",
        "print(f\"Test set accuracy: {test_score:.4f}\")\n",
        "\n",
        "y_pred = best_estimator.predict(X_test_scaled)\n",
        "test_score = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test set accuracy: {test_score:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717493928370
        },
        "editable": true,
        "run_control": {
          "frozen": false
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted',zero_division=1)\n",
        "recall = recall_score(y_test, y_pred, average='weighted',zero_division=1)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted',zero_division=1)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717493936425
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_report = classification_report(y_test, y_pred, target_names=[str(label) for label in label_mapping.keys()])\n",
        "print('Classification Report:\\n', class_report)\n",
        "\n",
        "# Plot the Classification Report\n",
        "report = classification_report(y_test, y_pred, target_names=[str(label) for label in label_mapping.keys()], output_dict=True)\n",
        "df_class_report = pd.DataFrame(report).transpose()\n",
        "\n",
        "# Plotting Precision, Recall, and F1-Score\n",
        "plt.figure(figsize=(10, 7))\n",
        "df_class_report.iloc[:-1, :-1].plot(kind='bar')\n",
        "plt.title('Classification Report')\n",
        "plt.ylabel('Scores')\n",
        "plt.xlabel('Labels')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717493939870
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix using Seaborn heatmap\n",
        "plt.figure(figsize=(7, 7))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=list(label_mapping.keys()), yticklabels=list(label_mapping.keys()))\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "\n",
        "# Display the heatmap\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1717495010527
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}